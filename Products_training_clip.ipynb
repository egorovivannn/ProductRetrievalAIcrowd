{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import faiss\n",
    "import copy\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "\n",
    "class ArcMarginProduct(nn.Module):\n",
    "    r\"\"\"Implement of large margin arc distance: :\n",
    "        Args:\n",
    "            in_features: size of each input sample\n",
    "            out_features: size of each output sample\n",
    "            s: norm of input feature\n",
    "            m: margin\n",
    "            cos(theta + m)\n",
    "        \"\"\"\n",
    "    def __init__(self, in_features, out_features, s=30.0, \n",
    "                 m=0.50, easy_margin=False, ls_eps=0.0, device=torch.device('cuda')):\n",
    "        super(ArcMarginProduct, self).__init__()\n",
    "        self.device = device\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.ls_eps = ls_eps  # label smoothing\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        # --------------------------- cos(theta) & phi(theta) ---------------------\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        # --------------------------- convert label to one-hot ---------------------\n",
    "        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n",
    "        one_hot = torch.zeros(cosine.size(), device=self.device)\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        if self.ls_eps > 0:\n",
    "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n",
    "        # -------------torch.where(out_i = {x_i if condition_i else y_i) ------------\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.s\n",
    "\n",
    "        return output\n",
    "\n",
    "class DenseCrossEntropy(nn.Module):\n",
    "    def forward(self, x, target):\n",
    "        x = x.float()\n",
    "        target = target.float()\n",
    "        logprobs = torch.nn.functional.log_softmax(x, dim=-1)\n",
    "\n",
    "        loss = -logprobs * target\n",
    "        loss = loss.sum(-1)\n",
    "        return loss.mean()\n",
    "\n",
    "class ArcMarginProduct_subcenter(nn.Module):\n",
    "    def __init__(self, in_features, out_features, k=3):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features*k, in_features))\n",
    "        self.reset_parameters()\n",
    "        self.k = k\n",
    "        self.out_features = out_features\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "    def forward(self, features):\n",
    "        cosine_all = F.linear(F.normalize(features), F.normalize(self.weight))\n",
    "        cosine_all = cosine_all.view(-1, self.out_features, self.k)\n",
    "        cosine, _ = torch.max(cosine_all, dim=2)\n",
    "        return cosine   \n",
    "\n",
    "class ArcFaceLossAdaptiveMargin(nn.modules.Module):\n",
    "    def __init__(self, margins, s=30.0):\n",
    "        super().__init__()\n",
    "        self.crit = DenseCrossEntropy()\n",
    "        self.s = s\n",
    "        self.margins = margins\n",
    "            \n",
    "    def forward(self, logits, labels, out_dim):\n",
    "        ms = []\n",
    "        ms = self.margins[labels.cpu().numpy()]\n",
    "        cos_m = torch.from_numpy(np.cos(ms)).float().cuda()\n",
    "        sin_m = torch.from_numpy(np.sin(ms)).float().cuda()\n",
    "        th = torch.from_numpy(np.cos(math.pi - ms)).float().cuda()\n",
    "        mm = torch.from_numpy(np.sin(math.pi - ms) * ms).float().cuda()\n",
    "        labels = F.one_hot(labels, out_dim).float()\n",
    "        logits = logits.float()\n",
    "        cosine = logits\n",
    "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
    "        phi = cosine * cos_m.view(-1,1) - sine * sin_m.view(-1,1)\n",
    "        phi = torch.where(cosine > th.view(-1,1), phi, cosine - mm.view(-1,1))\n",
    "        output = (labels * phi) + ((1.0 - labels) * cosine)\n",
    "        output *= self.s\n",
    "        loss = self.crit(output, labels)\n",
    "        return loss     \n",
    "\n",
    "def set_seed(seed):\n",
    "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.'''\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "\n",
    "def get_similiarity(embeddings, k):\n",
    "    print('Processing indices...')\n",
    "\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "\n",
    "    res = faiss.StandardGpuResources()\n",
    "\n",
    "    index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "\n",
    "    index.add(embeddings)\n",
    "\n",
    "    scores, indices = index.search(embeddings, k) \n",
    "    print('Finished processing indices')\n",
    "\n",
    "    return scores, indices\n",
    "\n",
    "def map_per_image(label, predictions): \n",
    "    try:\n",
    "        return 1 / (predictions[:5].index(label) + 1)\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "\n",
    "def map_per_set(labels, predictions):\n",
    "    return np.mean([map_per_image(l, p) for l,p in zip(labels, predictions)])\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self, window_size=None):\n",
    "        self.length = 0\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def reset(self):\n",
    "        self.length = 0\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        if self.window_size and (self.count >= self.window_size):\n",
    "            self.reset()\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def get_lr_groups(param_groups):\n",
    "        groups = sorted(set([param_g['lr'] for param_g in param_groups]))\n",
    "        groups = [\"{:2e}\".format(group) for group in groups]\n",
    "        return groups\n",
    "\n",
    "def convert_indices_to_labels(indices, labels):\n",
    "    indices_copy = copy.deepcopy(indices)\n",
    "    for row in indices_copy:\n",
    "        for j in range(len(row)):\n",
    "            row[j] = labels[row[j]]\n",
    "    return indices_copy\n",
    "\n",
    "class Multisample_Dropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Multisample_Dropout, self).__init__()\n",
    "        self.dropout = nn.Dropout(.1)\n",
    "        self.dropouts = nn.ModuleList([nn.Dropout((i+1)*.1) for i in range(5)])\n",
    "        \n",
    "    def forward(self, x, module):\n",
    "        x = self.dropout(x)\n",
    "        return torch.mean(torch.stack([module(dropout(x)) for dropout in self.dropouts],dim=0),dim=0) \n",
    "\n",
    "def transforms_auto_augment(image_path, image_size):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    train_transforms = transforms.Compose([transforms.AutoAugment(transforms.AutoAugmentPolicy.IMAGENET), transforms.PILToTensor()])\n",
    "    return train_transforms(image)\n",
    "\n",
    "def transforms_cutout(image_path, image_size):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.uint8)\n",
    "    train_transforms = A.Compose([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.ImageCompression(quality_lower=99, quality_upper=100),\n",
    "            A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=10, border_mode=0, p=0.7),\n",
    "            A.Resize(image_size, image_size),\n",
    "            A.Cutout(max_h_size=int(image_size * 0.4), max_w_size=int(image_size * 0.4), num_holes=1, p=0.5),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    return train_transforms(image=image)['image']\n",
    "\n",
    "def transforms_happy_whale(image_path, image_size):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.uint8)\n",
    "    aug8p3 = A.OneOf([\n",
    "            A.Sharpen(p=0.3),\n",
    "            A.ToGray(p=0.3),\n",
    "            A.CLAHE(p=0.3),\n",
    "        ], p=0.5)\n",
    "\n",
    "    train_transforms = A.Compose([\n",
    "            A.ShiftScaleRotate(rotate_limit=15, scale_limit=0.1, border_mode=cv2.BORDER_REFLECT, p=0.5),\n",
    "            A.Resize(image_size, image_size),\n",
    "            aug8p3,\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    return train_transforms(image=image)['image']\n",
    "\n",
    "def transforms_valid(image_path, image_size):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    valid_transforms = transforms.Compose([transforms.PILToTensor()]) \n",
    "    return valid_transforms(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import timm\n",
    "import math\n",
    "from transformers import (get_linear_schedule_with_warmup, \n",
    "                          get_cosine_schedule_with_warmup, \n",
    "                          get_cosine_with_hard_restarts_schedule_with_warmup,\n",
    "                          get_constant_schedule_with_warmup)\n",
    "from tqdm import tqdm\n",
    "import faiss\n",
    "import random\n",
    "import gc\n",
    "import transformers\n",
    "from transformers import CLIPProcessor, CLIPVisionModel,  CLIPVisionConfig\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from pytorch_metric_learning import losses\n",
    "import open_clip\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "    \n",
    "# df = pd.read_csv(df_path)\n",
    "class CFG:\n",
    "    model_name = 'ViT-B-16' \n",
    "    model_data = 'openai'\n",
    "    samples_per_class = 50\n",
    "    min_samples = 4\n",
    "    image_size = 336 \n",
    "    seed = 5\n",
    "    workers = 8\n",
    "    train_batch_size = 128\n",
    "    valid_batch_size = 128\n",
    "    emb_size = 512\n",
    "    vit_bb_lr = {'8': 1.25e-6, '16': 2.5e-6, '20': 5e-6, '24': 10e-6} \n",
    "    vit_bb_wd = 1e-3\n",
    "    hd_lr = 3e-4\n",
    "    hd_wd = 1e-5\n",
    "    autocast = True\n",
    "    n_warmup_steps = 30\n",
    "    n_epochs = 20\n",
    "    device = torch.device('cuda')\n",
    "    s=10.\n",
    "    m=.45\n",
    "    m_min=.05\n",
    "    acc_steps = 4\n",
    "    global_step = 0\n",
    "    train_csv_path = df_path\n",
    "    data_path = '../'\n",
    "    df = pd.read_csv(train_csv_path)\n",
    "    df = df[df['split']=='train'].reset_index(drop=True)\n",
    "    if datasets2exclude:\n",
    "        datasets2use = [i for i in df['dataset'].unique() if i not in datasets2exclude]\n",
    "        df = df[np.sum([df['dataset']==i for i in datasets2use], axis=0, dtype=bool)]\n",
    "        new_class = {i:idx for idx, i in enumerate(df['class'].unique())}\n",
    "        df['class'] = df['class'].map(new_class)\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "    n_classes = len(train_df)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_clip.list_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_backbone, model_transforms, _ = open_clip.create_model_and_transforms(CFG.model_name, pretrained=CFG.model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = model_transforms.transforms[0].size[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = model_transforms.transforms[-1].mean, model_transforms.transforms[-1].std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aug = A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.ImageCompression(quality_lower=50, quality_upper=100),\n",
    "        A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=10, border_mode=0, p=0.7),\n",
    "        A.Resize(image_size, image_size),\n",
    "        A.Cutout(max_h_size=int(image_size * 0.4), max_w_size=int(image_size * 0.4), num_holes=1, p=0.5),\n",
    "        A.OneOf([\n",
    "            A.ChannelShuffle(),\n",
    "            A.ChannelDropout(),\n",
    "            A.ColorJitter(),\n",
    "#             A.ISONoise(),\n",
    "            A.ToGray(),\n",
    "        ], p=0.65),\n",
    "        A.Normalize(mean=mean, std=std, p=1), \n",
    "    ])\n",
    "\n",
    "val_aug = A.Compose([\n",
    "        A.Resize(image_size, image_size),\n",
    "        A.Normalize(mean=mean, std=std, p=1)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class Products_dataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 df, \n",
    "                 mode='train', \n",
    "                 transform=None\n",
    "                 ):\n",
    "        self.df = df[df['split']==mode].reset_index(drop=True)\n",
    "        self.idx2class = {k:int(i) for i in self.df for k in self.df[i]['idxs']}\n",
    "        self.idxs = list(self.idx2class.keys())\n",
    "        self.augs = transform\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2class)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path = f'{self.idxs[idx]}.png'\n",
    "        label = self.idx2class[self.idxs[idx]]\n",
    "                    \n",
    "        try:\n",
    "\n",
    "            img = Image.open(path).convert('RGB')\n",
    "            img = np.array(img)\n",
    "            \n",
    "            if self.augs:\n",
    "                img = self.augs(image=img)['image']\n",
    "                img = torch.from_numpy(img).permute(2, 0, 1)\n",
    "        except Exception as e:\n",
    "            print(e, path)\n",
    "            img = torch.zeros((3, 224, 224))\n",
    "        \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, hidden_size, emb_size, n_classes):\n",
    "        super(Head, self).__init__()\n",
    "\n",
    "        self.emb = nn.Linear(hidden_size, emb_size, bias=False)\n",
    "        self.arc = ArcMarginProduct_subcenter(emb_size, n_classes)\n",
    "        self.dropout = Multisample_Dropout()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.dropout(x, self.emb)\n",
    "        \n",
    "        output = self.arc(embeddings)\n",
    "\n",
    "        return output, F.normalize(embeddings)\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    def __init__(self, vit_backbone):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        vit_backbone = vit_backbone.visual\n",
    "        self.img_size = vit_backbone.image_size\n",
    "        if type(self.img_size)==tuple:\n",
    "            self.img_size = self.img_size[1]\n",
    "        hidden_size = vit_backbone(torch.zeros((1, 3, self.img_size, self.img_size))).shape[1]\n",
    "        self.vit_backbone = vit_backbone\n",
    "        self.head = Head(hidden_size, CFG.emb_size, CFG.n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.vit_backbone(x)\n",
    "        return self.head(x)\n",
    "\n",
    "    def get_parameters(self):\n",
    "\n",
    "        parameter_settings = [] \n",
    "        parameter_settings.extend(self.get_parameter_section([(n, p) for n, p in self.vit_backbone.named_parameters()], lr=CFG.vit_bb_lr, wd=CFG.vit_bb_wd)) \n",
    "\n",
    "        parameter_settings.extend(self.get_parameter_section([(n, p) for n, p in self.head.named_parameters()], lr=CFG.hd_lr, wd=CFG.hd_wd)) \n",
    "\n",
    "        return parameter_settings\n",
    "\n",
    "    def get_parameter_section(self, parameters, lr=None, wd=None): \n",
    "        parameter_settings = []\n",
    "\n",
    "\n",
    "        lr_is_dict = isinstance(lr, dict)\n",
    "        wd_is_dict = isinstance(wd, dict)\n",
    "\n",
    "        layer_no = None\n",
    "        for no, (n,p) in enumerate(parameters):\n",
    "            \n",
    "            for split in n.split('.'):\n",
    "                if split.isnumeric():\n",
    "                    layer_no = int(split)\n",
    "            \n",
    "            if not layer_no:\n",
    "                layer_no = 0\n",
    "            \n",
    "            if lr_is_dict:\n",
    "                for k,v in lr.items():\n",
    "                    if layer_no < int(k):\n",
    "                        temp_lr = v\n",
    "                        break\n",
    "            else:\n",
    "                temp_lr = lr\n",
    "\n",
    "            if wd_is_dict:\n",
    "                for k,v in wd.items():\n",
    "                    if layer_no < int(k):\n",
    "                        temp_wd = v\n",
    "                        break\n",
    "            else:\n",
    "                temp_wd = wd\n",
    "\n",
    "            weight_decay = 0.0 if 'bias' in n else temp_wd\n",
    "\n",
    "            parameter_setting = {\"params\" : p, \"lr\" : temp_lr, \"weight_decay\" : temp_wd}\n",
    "\n",
    "            parameter_settings.append(parameter_setting)\n",
    "\n",
    "            #print(f'no {no} | params {n} | lr {temp_lr} | weight_decay {weight_decay} | requires_grad {p.requires_grad}')\n",
    "\n",
    "        return parameter_settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Products_dataset(\n",
    "        train_df,\n",
    "        mode='train', \n",
    "        transform=train_aug,\n",
    "    )\n",
    "train_loader = DataLoader(train_ds, num_workers=8, batch_size=CFG.train_batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "valid_ds = Products_dataset(\n",
    "        valid_df,\n",
    "        mode='valid', \n",
    "        transform=val_aug,\n",
    "    )\n",
    "valid_loader = DataLoader(valid_ds, num_workers=8, batch_size=CFG.valid_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ArcFace_criterion(logits_m, target, margins):\n",
    "    arc = ArcFaceLossAdaptiveMargin(margins=margins, s=CFG.s)\n",
    "    loss_m = arc(logits_m, target, CFG.n_classes)\n",
    "    return loss_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, scaler, scheduler, epoch, pbar_draw=False):\n",
    "    model.train()\n",
    "    loss_metrics = AverageMeter()\n",
    "    criterion = ArcFace_criterion\n",
    "\n",
    "    tmp = np.sqrt(1 / np.sqrt(value_counts))\n",
    "    margins = (tmp - tmp.min()) / (tmp.max() - tmp.min()) * CFG.m + CFG.m_min\n",
    "        \n",
    "    bar = tqdm(train_loader, disable=not pbar_draw)\n",
    "#     bar = tqdm(train_loader, disable=False)\n",
    "    for step, data in enumerate(bar):\n",
    "        step += 1\n",
    "        images = data[0].to(CFG.device, dtype=torch.float)\n",
    "        labels = data[1].to(CFG.device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.autocast):\n",
    "            outputs, features = model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels, margins)\n",
    "        loss_metrics.update(loss.item(), batch_size)\n",
    "        loss = loss / CFG.acc_steps\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if step % CFG.acc_steps == 0 or step == len(bar):\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "            CFG.global_step += 1\n",
    "            \n",
    "        lrs = get_lr_groups(optimizer.param_groups)\n",
    "\n",
    "        loss_avg = loss_metrics.avg\n",
    "\n",
    "        bar.set_postfix(loss=loss_avg, epoch=epoch, lrs=lrs, step=CFG.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "def map_per_image(label, predictions, k=5):\n",
    "    \"\"\"Computes the precision score of one image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    label : string\n",
    "            The true label of the image\n",
    "    predictions : list\n",
    "            A list of predicted elements (order does matter, 5 predictions allowed per image)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "    \"\"\"    \n",
    "    try:\n",
    "        return 1 / (predictions[:k].index(label) + 1)\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "\n",
    "def map_per_set(labels, predictions, k=5):\n",
    "    \"\"\"Computes the average over multiple images.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    labels : list\n",
    "             A list of the true labels. (Only one true label per images allowed!)\n",
    "    predictions : list of list\n",
    "             A list of predicted elements (order does matter, 5 predictions allowed per image)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "    \"\"\"\n",
    "    return np.mean([map_per_image(l, p, k=k) for l,p in zip(labels, predictions)])\n",
    "\n",
    "\n",
    "def validate(model, loader, embedding_size=512):\n",
    "    model.eval()\n",
    "    device = 'cuda'\n",
    "    bd, labels = [], []\n",
    "    for i, (images, label) in tqdm(enumerate(loader), total=len(loader)):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images.to(device))[1].cpu().detach().tolist()\n",
    "        bd.extend(outputs)\n",
    "        labels.extend(label)\n",
    "    bd = np.array(bd, np.float32)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    index = faiss.IndexFlatL2(embedding_size)\n",
    "    index.add(bd)\n",
    "    D, I = index.search(bd, 6)\n",
    "    global_preds = labels[I[:, 1:]]\n",
    "    global_labels = labels\n",
    "    acc_1 = map_per_set(global_labels.tolist(), global_preds.tolist(), k=1) \n",
    "    acc_5 = map_per_set(global_labels.tolist(), global_preds.tolist(), k=5)\n",
    "    print(f'Metric mAP@1 = {acc_1}, mAP@5 = {acc_5}')\n",
    "    return acc_1, acc_5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = [df[i]['cnt'] for i in df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vit_backbone, model_transforms, _ = open_clip.create_model_and_transforms(CFG.model_name, pretrained=CFG.model_data)\n",
    "model = Model(vit_backbone.cpu()).to(CFG.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate(model, valid_loader, embedding_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.get_parameters())\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=CFG.autocast)\n",
    "steps_per_epoch = math.ceil(len(train_loader) / CFG.acc_steps)\n",
    "num_training_steps = math.ceil(CFG.n_epochs * steps_per_epoch)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer,\n",
    "                                            num_training_steps=num_training_steps,\n",
    "                                            num_warmup_steps=CFG.n_warmup_steps)   \n",
    "CFG.global_step = 0                   \n",
    "for epoch in tqdm(range(math.ceil(CFG.n_epochs))):\n",
    "\n",
    "    train(model, train_loader, optimizer, scaler, scheduler, epoch, pbar_draw=True)\n",
    "    score = validate(model, valid_loader, embedding_size=512)\n",
    "    print(f'Epoch = {epoch}, score:', score)\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "def objective(trial):\n",
    "    # Generate the model.\n",
    "\n",
    "    CFG.s = trial.suggest_int(\"s\", 5, 90, step=5)\n",
    "    CFG.hd_lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    model = Model(vit_backbone.cpu()).to(CFG.device)\n",
    "    \n",
    "    CFG.n_epochs = trial.suggest_int(\"n_epochs\", 1, 11, step=2)\n",
    "#     CFG.n_epochs = 10\n",
    "    CFG.n_warmup_steps = trial.suggest_int('n_warmup_steps', 1, 5)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.get_parameters())\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.autocast)\n",
    "    steps_per_epoch = math.ceil(len(train_loader) / CFG.acc_steps)\n",
    "    num_training_steps = math.ceil(CFG.n_epochs * steps_per_epoch)\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer,\n",
    "                                                num_training_steps=num_training_steps,\n",
    "                                                num_warmup_steps=CFG.n_warmup_steps)   \n",
    "    CFG.global_step = 0                   \n",
    "    for epoch in range(math.ceil(CFG.n_epochs)):\n",
    "\n",
    "        train(model, train_loader, optimizer, scaler, scheduler, epoch)\n",
    "        score = validate(model, valid_loader)\n",
    "#         print(f'Epoch = {epoch}, score:', score)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        trial.report(score, epoch)\n",
    "\n",
    "        # Handle pruning based on the intermediate value.\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=20, timeout=None)\n",
    "\n",
    "    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "    print(\"Study statistics: \")\n",
    "    print(\"  Number of finished trials: \", len(study.trials))\n",
    "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "    print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: \", trial.value)\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4ee870ab444af8a8689fba9fdb6a16993f9af4d6f8c51486b98fd7ee4129479"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
